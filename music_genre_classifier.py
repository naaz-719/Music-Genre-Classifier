# -*- coding: utf-8 -*-
"""Music Genre Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wodUFg0YqcHtzEubv2XePraETRSsZBtW
"""

!pip install python_speech_features

!pip install lime shap scikit-learn pandas

!pip install kagglehub

import kagglehub

# Download latest version
path = kagglehub.dataset_download("andradaolteanu/gtzan-dataset-music-genre-classification")

print("Path to dataset files:", path)

#Import and setup
import numpy as np
import pandas as pd
import pandas as pd
import matplotlib.pyplot as plt
import scipy.io.wavfile as wav
from python_speech_features import mfcc
import seaborn as sns
import librosa.display
import librosa
from tempfile import TemporaryFile
from sklearn.preprocessing import normalize

import os
import math
import pickle
import random
import operator


import math
from PIL import Image
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Additional libraries for data preprocessing and visualization
import cv2
from sklearn.model_selection import train_test_split



#To download the dataset correctrly

dataset_path = kagglehub.dataset_download("andradaolteanu/gtzan-dataset-music-genre-classification")

print("‚úÖ Dataset downloaded.")
print("üìÅ Path to dataset files:", dataset_path)

# Set working directory to dataset folder
# os.chdir(dataset_path) # No need to change the working directory

# Path to audio and metadata
audio_data_path = os.path.join(dataset_path, "genres_original")
csv_file_path = os.path.join(dataset_path, "features_30_sec.csv") # This path seems incorrect. Let's find the correct path.

# Load CSV metadata
# music_data = pd.read_csv(csv_file_path) # Commenting out for now
# music_data.head() # Commenting out for now

# Let's list the files in the dataset directory to find the correct path
print("Files in dataset directory:")
for root, dirs, files in os.walk(dataset_path):
    for name in files:
        print(os.path.join(root, name))

#To find the count of the Genre

# Correct CSV path
csv_file_path = os.path.join(dataset_path, "Data", "features_30_sec.csv")

# Load it into a DataFrame
music_data = pd.read_csv(csv_file_path)

# Now this will work:
music_data['label'].value_counts()

"""**Exploratory Data Analysis**"""

import librosa.display
import librosa
from importlib import reload
plt=reload(plt)

# Use the correct base path from kagglehub
base_path = os.path.join(dataset_path, "Data", "genres_original")

# List of genre folders
genres = ['blues', 'classical', 'country', 'disco', 'hiphop',
          'jazz', 'metal', 'pop', 'reggae', 'rock']

# Create a figure for plotting
plt.figure(figsize=(18, 10))

# Loop through each genre and plot the waveform
for i, genre in enumerate(genres):
    genre_folder = os.path.join(base_path, genre)
    audio_files = os.listdir(genre_folder)

    if audio_files:  # Only proceed if files are present
        path = os.path.join(genre_folder, audio_files[0])

        plt.subplot(5, 2, i + 1)
        x, sr = librosa.load(path, duration=30)
        librosa.display.waveshow(x, sr=sr)

        plt.xlabel('Time')
        plt.ylabel('Amplitude')
        plt.title(f'Waveform of {genre.capitalize()}')

plt.tight_layout()
plt.show()

"""## **Heatmap**"""

# Computing the Correlation Matrix
spike_cols = [col for col in music_data.columns if 'mean' in col]

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(16, 11));

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(music_data[spike_cols].corr(), cmap='YlGn')

plt.title('Heatmap for MEAN variables', fontsize = 20)
plt.xticks(fontsize = 10)
plt.yticks(fontsize = 10)
plt.show()

"""### **SPECTOGRAM**"""

# Correct path to the audio file in the kagglehub-downloaded dataset
audio_file = os.path.join(dataset_path, "Data", "genres_original", "blues", "blues.00000.wav")

# Load the audio
data, sr = librosa.load(audio_file)

# Compute STFT
stft = librosa.stft(data)
stft_db = librosa.amplitude_to_db(abs(stft))

# Plot spectrogram
plt.figure(figsize=(14, 6))
librosa.display.specshow(stft_db, sr=sr, x_axis='time', y_axis='hz')
plt.ylim(0, 200)  # Focus on low frequencies
plt.title('Spectrogram for Genre: Blues')
plt.colorbar(format='%+2.0f dB')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import librosa.display

spectral_rolloff = librosa.feature.spectral_rolloff(y=data, sr=sr)[0]

plt.figure(figsize=(12, 6))
librosa.display.waveshow(data, sr=sr, alpha=0.4, color="#2B4F72", label='Waveform')
plt.plot(librosa.times_like(spectral_rolloff), spectral_rolloff, color='r', label='Spectral Rolloff')
plt.legend()
plt.title("Waveform and Spectral Rolloff")
plt.show()

chroma = librosa.feature.chroma_stft(y = data, sr = sr)
plt.figure(figsize = (16,6))
librosa.display.specshow(chroma, sr = sr, x_axis='time', y_axis='chroma', cmap='coolwarm')
plt.colorbar()
plt.title('Chroma Feature for Genre Blue')
plt.show()

mfccs = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13)

# Display the MFCCs
plt.figure(figsize=(10, 4))
librosa.display.specshow(mfccs, x_axis='time', sr=sr)
plt.colorbar(format='%+2.0f dB')
plt.show()

"""# **KNN** : To find the distance between the 2 vectors"""

def getNeighbors(trainingSet, instance, k):
    distances = []
    for x in range (len(trainingSet)):
        dist = distance(trainingSet[x], instance, k )+ distance(instance, trainingSet[x], k)
        distances.append((trainingSet[x][2], dist))
    distances.sort(key=operator.itemgetter(1))
    neighbors = []
    for x in range(k):
        neighbors.append(distances[x][0])
    return neighbors

#Identify the nearest neighbours

def nearestClass(neighbors):
    classVote = {}

    for x in range(len(neighbors)):
        response = neighbors[x]
        if response in classVote:
            classVote[response]+=1
        else:
            classVote[response]=1

    sorter = sorted(classVote.items(), key = operator.itemgetter(1), reverse=True)
    return sorter[0][0]

#Define a function for model evaluation


def getAccuracy(testSet, predictions):
    correct = 0
    for x in range (len(testSet)):
        if testSet[x][-1]==predictions[x]:
            correct+=1
    return 1.0*correct/len(testSet)

import os

root_path = '/kaggle/input/gtzan-dataset-music-genre-classification'
print("Root contents:", os.listdir(root_path))

audio_data_path = os.path.join(root_path, 'Data', 'genres_original')
print("Genres available:", os.listdir(audio_data_path))

import os
import pickle
import numpy as np
import scipy.io.wavfile as wav
from python_speech_features import mfcc

# Correct root path for GTZAN dataset inside Kaggle environment
audio_data_path = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original'

print("Genres available:", os.listdir(audio_data_path))

f = open("mydataset.dat", "wb")
i = 0

for folder in os.listdir(audio_data_path):
    i += 1
    if i == 11:  # process only first 10 genres
        break

    folder_path = os.path.join(audio_data_path, folder)
    for file in os.listdir(folder_path):
        try:
            file_path = os.path.join(folder_path, file)
            rate, sig = wav.read(file_path)
            mfcc_feat = mfcc(sig, rate, winlen=0.020, appendEnergy=False)
            covariance = np.cov(mfcc_feat.T)
            mean_matrix = mfcc_feat.mean(0)
            feature = (mean_matrix, covariance, i)
            pickle.dump(feature, f)
        except Exception as e:
            print(f"Got an exception: {e} in folder: {folder} filename: {file}")

f.close()

#Train and Test on dataset

dataset = []

def loadDataset(filename, split, trset, teset):
    with open('mydataset.dat','rb') as f:
        while True:
            try:
                dataset.append(pickle.load(f))
            except EOFError:
                f.close()
                break
    for x in range(len(dataset)):
        if random.random() < split:
            trset.append(dataset[x])
        else:
            teset.append(dataset[x])

trainingSet = []
testSet = []
loadDataset(music_data, 0.68, trainingSet, testSet)

def distance(instance1, instance2, k):
    distance = 0
    mm1 = instance1[0]
    cm1 = instance1[1]
    mm2 = instance2[0]
    cm2 = instance2[1]
    distance = np.trace(np.dot(np.linalg.inv(cm2), cm1))
    distance += (np.dot(np.dot((mm2-mm1).transpose(), np.linalg.inv(cm2)), mm2-mm1))
    distance += np.log(np.linalg.det(cm2)) - np.log(np.linalg.det(cm1))
    distance -= k
    return distance

# Make the prediction using KNN(K nearest Neighbors)
length = len(testSet)
predictions = []
for x in range(length):
    predictions.append(nearestClass(getNeighbors(trainingSet, testSet[x], 5)))

accuracy1 = getAccuracy(testSet, predictions)
print(accuracy1)

#Testing

from collections import defaultdict
results = defaultdict(int)

directory = audio_data_path
i = 1
for folder in os.listdir(directory):
    results[i] = folder
    i += 1


pred = nearestClass(getNeighbors(dataset, feature, 5))
print(results[pred])

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
# Load the dataset from your given path
path = kagglehub.dataset_download("andradaolteanu/gtzan-dataset-music-genre-classification")
data = pd.read_csv(os.path.join(path, 'Data', 'features_30_sec.csv'))
data = data.iloc[0:, 1:]
y = data['label']
X = data.loc[:, data.columns != 'label']
cols = X.columns
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
scaled_df = pd.DataFrame(np_scaled, columns = cols)
scaled_df.head()

# Assuming X contains your features and y contains the corresponding labels
X_train, X_test, y_train, y_test = train_test_split(scaled_df, y, test_size=0.2, random_state=42)

# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
class_report = classification_report(y_test, y_pred)
print("\nClassification Report:")
print(class_report)

#Hyperparameter Tuning

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd

# Define the parameter grid for tuning
param_grid = {
    'n_estimators': [50, 100, 200],      # Number of trees in the forest
    'max_depth': [None, 10, 20, 30],     # Maximum depth of the tree
}

# Initialize base model
rf_model = RandomForestClassifier(random_state=42)

# Apply GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Extract best parameters and retrain model
best_params = grid_search.best_params_
best_rf_model = RandomForestClassifier(**best_params, random_state=42)
best_rf_model.fit(X_train, y_train)

# Print the best hyperparameters
print("\n=== Best Random Forest Model Parameters ===")
for param, value in best_params.items():
    print(f"{param}: {value}")

# Evaluate performance
train_acc = accuracy_score(y_train, best_rf_model.predict(X_train))
test_acc = accuracy_score(y_test, best_rf_model.predict(X_test))

print(f"\nTraining Accuracy: {train_acc:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")

# Optional: show top feature importances
if hasattr(best_rf_model, 'feature_importances_'):
    importances = pd.Series(best_rf_model.feature_importances_, index=X_train.columns)
    print("\n=== Top 10 Feature Importances ===")
    print(importances.sort_values(ascending=False).head(10))

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from lime.lime_tabular import LimeTabularExplainer
import shap

# Download dataset path via kagglehub
path = kagglehub.dataset_download("andradaolteanu/gtzan-dataset-music-genre-classification")

# Load the CSV file, skip the first index column if present
data = pd.read_csv(os.path.join(path, 'Data', 'features_30_sec.csv'))
data = data.iloc[:, 1:]  # Remove the first column (index)

# Separate features and labels
X = data.drop(columns=['label'])
y = data['label']
feature_names = X.columns.tolist()
class_names = y.unique().tolist()

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Train KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
knn_preds = knn_model.predict(X_test)
rf_preds = rf_model.predict(X_test)

# Print accuracy scores
print(f"KNN Accuracy: {accuracy_score(y_test, knn_preds):.2f}")
print(f"Random Forest Accuracy: {accuracy_score(y_test, rf_preds):.2f}")

# Initialize LIME explainers
knn_explainer = LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=feature_names,
    class_names=class_names,
    discretize_continuous=True
)

rf_explainer = LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=feature_names,
    class_names=class_names,
    discretize_continuous=True
)

# Function wrappers for predict_proba with correct DataFrame input for LIME
def knn_predict_proba(x):
    df = pd.DataFrame(x, columns=feature_names)
    return knn_model.predict_proba(df)

def rf_predict_proba(x):
    df = pd.DataFrame(x, columns=feature_names)
    return rf_model.predict_proba(df)

# Explain first test instance using LIME
knn_exp = knn_explainer.explain_instance(
    data_row=X_test.iloc[0].values,
    predict_fn=knn_predict_proba,
    num_features=10
)

rf_exp = rf_explainer.explain_instance(
    data_row=X_test.iloc[0].values,
    predict_fn=rf_predict_proba,
    num_features=10
)

# Print LIME explanations nicely
print("\nLIME Explanation for KNN model (first test instance):")
for feature, weight in knn_exp.as_list():
    print(f"{feature}: {weight:.4f}")

print("\nLIME Explanation for Random Forest model (first test instance):")
for feature, weight in rf_exp.as_list():
    print(f"{feature}: {weight:.4f}")

# SHAP explanation for Random Forest
print("\nComputing SHAP values for Random Forest model...")

# Create SHAP explainer and calculate SHAP values
shap_explainer = shap.TreeExplainer(rf_model)
shap_values = shap_explainer.shap_values(X_test)

# Display SHAP summary plot (bar plot for feature importance)
shap.summary_plot(shap_values, X_test, plot_type="bar", show=True)

pip install playsound

# User inputs a music genre, then 5 random music files from that genre are selected and played

import os
import random
from IPython.display import Audio, display

# Root path to GTZAN audio dataset
audio_data_path = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original'

def play_multiple_genre_audios(genre_name, num_audios=5):
    genre_folder = os.path.join(audio_data_path, genre_name.lower())
    if not os.path.exists(genre_folder):
        print(f"Genre '{genre_name}' not found. Available genres:")
        print(os.listdir(audio_data_path))
        return

    audio_files = [f for f in os.listdir(genre_folder) if f.endswith('.wav')]
    if not audio_files:
        print(f"No audio files found in genre folder '{genre_name}'.")
        return

    # Pick num_audios random audio files without replacement (or all if less than num_audios)
    selected_files = random.sample(audio_files, min(num_audios, len(audio_files)))

    print(f"Playing {len(selected_files)} random audio files from genre '{genre_name}':")

    for audio_file in selected_files:
        audio_path = os.path.join(genre_folder, audio_file)
        print(f"Playing: {audio_file}")
        display(Audio(audio_path))
        # Optional: pause between audios or just play inline one after another

# Input from user
genre = input("Enter a genre (e.g., rock, blues, classical): ").strip()
play_multiple_genre_audios(genre)

#Updated model accuracy

# Define the parameter grid to search over
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30]
}

# Initialize the Random Forest model
rf = RandomForestClassifier(random_state=42)

# Setup GridSearchCV with 5-fold cross-validation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Fit on training data
grid_search.fit(X_train, y_train)

# Best parameters found
print("Best Parameters:", grid_search.best_params_)

# Use the best model to predict test data
best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(X_test)

# Calculate and print accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Updated Model Accuracy: {accuracy * 100:.2f}%")